{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels\n",
    "toxic_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "#train dataset\n",
    "train_set = pd.read_csv('train.csv')\n",
    "\n",
    "#train dataset split\n",
    "train_corpus = train_set.drop( toxic_columns, axis=1)\n",
    "train_corpus = train_corpus.drop( ['id'], axis=1)\n",
    "train_labels = train_set.drop( ['id','comment_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus['comment_text'].fillna(value='none', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text\n",
       "0  Explanation\\nWhy the edits made under my usern...\n",
       "1  D'aww! He matches this background colour I'm s...\n",
       "2  Hey man, I'm really not trying to edit war. It...\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4  You, sir, are my hero. Any chance you remember..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def normalize_text(comment):\n",
    "    comment = comment.lower() \n",
    "    comment = re.sub(r\"i'm\", \"i am \", comment)\n",
    "    comment = re.sub(r\"\\'s\", \" \", comment)\n",
    "    comment = re.sub(r\"\\'ve\", \" have \", comment)\n",
    "    comment = re.sub(r\"can't\", \"can not \", comment)\n",
    "    comment = re.sub(r\"n't\", \" not \", comment)\n",
    "    comment = re.sub(r\"\\'ll\", \" will \", comment)\n",
    "    comment = re.sub(r\"\\'re\", \" are \", comment)\n",
    "    comment = re.sub(r\"\\'d\", \" would \", comment)\n",
    "    comment = re.sub(r\"what's\", \"what is \", comment)\n",
    "    comment = re.sub(r\"\\'scuse\", \" excuse \", comment)\n",
    "    comment = re.sub(\"[^a-z]\", \" \", comment)\n",
    "    comment = comment.strip(' ')\n",
    "    for word in stop_words: #removing stopwords\n",
    "        token = \" \" + word + \" \"\n",
    "        comment = comment.replace(token, \" \")\n",
    "        comment = comment.replace(\"  \", \" \")\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus['comment_text'] = train_corpus['comment_text'].map(lambda el : normalize_text(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cocksucker piss around work'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus['comment_text'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer( analyzer='word', ngram_range=(1, 1))\n",
    "corpus = vectorizer.fit_transform(train_corpus['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.toarray()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['accidents', 'appears', 'article', 'articles', 'backlog', 'date',\n",
       "        'delay', 'eg', 'else', 'etc', 'exact', 'first', 'form', 'format',\n",
       "        'formatting', 'good', 'guess', 'ie', 'improvement', 'know',\n",
       "        'later', 'let', 'listed', 'make', 'may', 'more', 'need',\n",
       "        'nominations', 'one', 'please', 'preferences', 'real',\n",
       "        'references', 'relevant', 'review', 'reviewer', 'section',\n",
       "        'statistics', 'style', 'subsection', 'suggestions', 'think',\n",
       "        'tidying', 'transport', 'turns', 'types', 'want', 'wikipedia',\n",
       "        'wondered'], dtype='<U4955')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(corpus.toarray()[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'more make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later one else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipedia good article nominations transport'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus['comment_text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset \n",
    "sub_corpus = pd.read_csv('test.csv')\n",
    "sub_corpus['comment_text'] = sub_corpus['comment_text'].map(lambda el : normalize_text(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_vect=vectorizer.transform(sub_corpus['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category : toxic\n",
      "--- NaiveBayes model accuracy is 0.920037599874667\n",
      "--- LogisticRegression model accuracy is 0.955068149772834\n",
      "--- RandomForest model accuracy is 0.902835657214476\n",
      "Category : severe_toxic\n",
      "--- NaiveBayes model accuracy is 0.9896287012376626\n",
      "--- LogisticRegression model accuracy is 0.9901613661287796\n",
      "--- RandomForest model accuracy is 0.989691367695441\n",
      "Category : obscene\n",
      "--- NaiveBayes model accuracy is 0.9523734920883598\n",
      "--- LogisticRegression model accuracy is 0.9766567444775184\n",
      "--- RandomForest model accuracy is 0.9467961773460755\n",
      "Category : threat\n",
      "--- NaiveBayes model accuracy is 0.997086009713301\n",
      "--- LogisticRegression model accuracy is 0.9973053423155256\n",
      "--- RandomForest model accuracy is 0.9971486761710794\n",
      "Category : insult\n",
      "--- NaiveBayes model accuracy is 0.9516841610527965\n",
      "--- LogisticRegression model accuracy is 0.9691054363152123\n",
      "--- RandomForest model accuracy is 0.9500548331505562\n",
      "Category : identity_hate\n",
      "--- NaiveBayes model accuracy is 0.9906000313332289\n",
      "--- LogisticRegression model accuracy is 0.991414695284349\n",
      "--- RandomForest model accuracy is 0.9906626977910074\n"
     ]
    }
   ],
   "source": [
    "models_bench = { 'NaiveBayes': MultinomialNB(),\n",
    "                 'LogisticRegression': LogisticRegression(solver='sag'),\n",
    "                 'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=3)\n",
    "               }\n",
    "\n",
    "submission_df_NB = sub_corpus.drop(['comment_text'], axis=1)\n",
    "submission_df_LR = sub_corpus.drop(['comment_text'], axis=1)\n",
    "submission_df_RF = sub_corpus.drop(['comment_text'], axis=1)\n",
    "\n",
    "df_x = train_corpus.comment_text\n",
    "for category in toxic_columns:\n",
    "    print('Category : {}'.format(category))\n",
    "    df_y = train_labels[category]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=0)\n",
    "    x_train_tf=vectorizer.transform(x_train)\n",
    "    x_test_tf=vectorizer.transform(x_test)\n",
    "    for key in models_bench:\n",
    "        clf=models_bench[key]\n",
    "        clf.fit(x_train_tf,y_train)\n",
    "        predictions = clf.predict(x_test_tf)\n",
    "        print('--- {} model accuracy is {}'.format(key,accuracy_score(y_test, predictions)))\n",
    "        if key == 'NaiveBayes': \n",
    "            submission_df_NB.loc[:,category] = clf.predict_proba(sub_vect)[:,1]\n",
    "        if key == 'LogisticRegression': \n",
    "            submission_df_LR.loc[:,category] = clf.predict_proba(sub_vect)[:,1]\n",
    "        if key == 'RandomForest': \n",
    "            submission_df_RF.loc[:,category] = clf.predict_proba(sub_vect)[:,1]\n",
    "            \n",
    "submission_df_NB.to_csv('submission_NB.csv', index=False)\n",
    "submission_df_LR.to_csv('submission_LR.csv', index=False)\n",
    "submission_df_RF.to_csv('submission_RF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle NB score -> 0.85244\n",
      "Kaggle LR score -> 0.97424\n",
      "Kaggle RF score -> 0.82695\n"
     ]
    }
   ],
   "source": [
    "print('Kaggle NB score -> 0.85244')\n",
    "print('Kaggle LR score -> 0.97424')\n",
    "print('Kaggle RF score -> 0.82695')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category : toxic\n",
      "--- NaiveBayes model\n",
      "--- --- accuracy is 0.920037599874667\n",
      "--- --- roc-auc  is 0.8791334650084888\n",
      "--- LogisticRegression model\n",
      "--- --- accuracy is 0.955068149772834\n",
      "--- --- roc-auc  is 0.9709632768584783\n",
      "--- RandomForest model\n",
      "--- --- accuracy is 0.902835657214476\n",
      "--- --- roc-auc  is 0.8782164703831514\n",
      "Category : severe_toxic\n",
      "--- NaiveBayes model\n",
      "--- --- accuracy is 0.9896287012376626\n",
      "--- --- roc-auc  is 0.8874575458289492\n",
      "--- LogisticRegression model\n",
      "--- --- accuracy is 0.9901613661287796\n",
      "--- --- roc-auc  is 0.9855080845520996\n",
      "--- RandomForest model\n",
      "--- --- accuracy is 0.989691367695441\n",
      "--- --- roc-auc  is 0.8968909988015543\n",
      "Category : obscene\n",
      "--- NaiveBayes model\n",
      "--- --- accuracy is 0.9523734920883598\n",
      "--- --- roc-auc  is 0.8812767565492993\n",
      "--- LogisticRegression model\n",
      "--- --- accuracy is 0.9766567444775184\n",
      "--- --- roc-auc  is 0.9837537239955682\n",
      "--- RandomForest model\n",
      "--- --- accuracy is 0.9467961773460755\n",
      "--- --- roc-auc  is 0.8711489347586419\n",
      "Category : threat\n",
      "--- NaiveBayes model\n",
      "--- --- accuracy is 0.997086009713301\n",
      "--- --- roc-auc  is 0.7678778612036532\n",
      "--- LogisticRegression model\n",
      "--- --- accuracy is 0.9973053423155256\n",
      "--- --- roc-auc  is 0.9844570964480467\n",
      "--- RandomForest model\n",
      "--- --- accuracy is 0.9971486761710794\n",
      "--- --- roc-auc  is 0.7584895151354426\n",
      "Category : insult\n",
      "--- NaiveBayes model\n",
      "--- --- accuracy is 0.9516841610527965\n",
      "--- --- roc-auc  is 0.8724510349051846\n",
      "--- LogisticRegression model\n",
      "--- --- accuracy is 0.9691054363152123\n",
      "--- --- roc-auc  is 0.9725398296777387\n",
      "--- RandomForest model\n",
      "--- --- accuracy is 0.9500548331505562\n",
      "--- --- roc-auc  is 0.8836885310448795\n",
      "Category : identity_hate\n",
      "--- NaiveBayes model\n",
      "--- --- accuracy is 0.9906000313332289\n",
      "--- --- roc-auc  is 0.8212709669188671\n",
      "--- LogisticRegression model\n",
      "--- --- accuracy is 0.991414695284349\n",
      "--- --- roc-auc  is 0.9785415118406481\n",
      "--- RandomForest model\n",
      "--- --- accuracy is 0.9906626977910074\n",
      "--- --- roc-auc  is 0.8899153840651097\n"
     ]
    }
   ],
   "source": [
    "metrics = defaultdict()\n",
    "models_bench = { 'NaiveBayes': MultinomialNB(),\n",
    "                 'LogisticRegression': LogisticRegression(solver='sag'),\n",
    "                 'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=3)\n",
    "               }\n",
    "\n",
    "submission_df_NB = sub_corpus.drop(['comment_text'], axis=1)\n",
    "submission_df_LR = sub_corpus.drop(['comment_text'], axis=1)\n",
    "submission_df_RF = sub_corpus.drop(['comment_text'], axis=1)\n",
    "\n",
    "df_x = train_corpus.comment_text\n",
    "\n",
    "for category in toxic_columns:\n",
    "    print('Category : {}'.format(category))\n",
    "    metrics[category] = {}\n",
    "    df_y = train_labels[category]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=0)\n",
    "    x_train_tf=vectorizer.transform(x_train)\n",
    "    x_test_tf=vectorizer.transform(x_test)\n",
    "    for key in models_bench:\n",
    "        metrics[category][key] = {}\n",
    "        clf=models_bench[key]\n",
    "        clf.fit(x_train_tf,y_train)\n",
    "        predictions = clf.predict(x_test_tf)\n",
    "        predictions_proba = clf.predict_proba(x_test_tf)\n",
    "        metrics[category][key]['accuracy'] = accuracy_score(y_test, predictions)\n",
    "        metrics[category][key]['roc-auc'] = roc_auc_score(y_test, predictions_proba[:,1])\n",
    "        print('--- {} model'.format(key))\n",
    "        print('--- --- accuracy is {}'.format(metrics[category][key]['accuracy']))\n",
    "        print('--- --- roc-auc  is {}'.format(metrics[category][key]['roc-auc']))\n",
    "        if key == 'NaiveBayes': \n",
    "            submission_df_NB.loc[:,category] = clf.predict_proba(sub_vect)[:,1]\n",
    "        if key == 'LogisticRegression': \n",
    "            submission_df_LR.loc[:,category] = clf.predict_proba(sub_vect)[:,1]\n",
    "        if key == 'RandomForest': \n",
    "            submission_df_RF.loc[:,category] = clf.predict_proba(sub_vect)[:,1]\n",
    "            \n",
    "submission_df_NB.to_csv('submission_NB.csv', index=False)\n",
    "submission_df_LR.to_csv('submission_LR.csv', index=False)\n",
    "submission_df_RF.to_csv('submission_RF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'toxic': {'NaiveBayes': {'accuracy': 0.920037599874667,\n",
       "               'roc-auc': 0.8791334650084888},\n",
       "              'LogisticRegression': {'accuracy': 0.955068149772834,\n",
       "               'roc-auc': 0.9709632768584783},\n",
       "              'RandomForest': {'accuracy': 0.902835657214476,\n",
       "               'roc-auc': 0.8782164703831514}},\n",
       "             'severe_toxic': {'NaiveBayes': {'accuracy': 0.9896287012376626,\n",
       "               'roc-auc': 0.8874575458289492},\n",
       "              'LogisticRegression': {'accuracy': 0.9901613661287796,\n",
       "               'roc-auc': 0.9855080845520996},\n",
       "              'RandomForest': {'accuracy': 0.989691367695441,\n",
       "               'roc-auc': 0.8968909988015543}},\n",
       "             'obscene': {'NaiveBayes': {'accuracy': 0.9523734920883598,\n",
       "               'roc-auc': 0.8812767565492993},\n",
       "              'LogisticRegression': {'accuracy': 0.9766567444775184,\n",
       "               'roc-auc': 0.9837537239955682},\n",
       "              'RandomForest': {'accuracy': 0.9467961773460755,\n",
       "               'roc-auc': 0.8711489347586419}},\n",
       "             'threat': {'NaiveBayes': {'accuracy': 0.997086009713301,\n",
       "               'roc-auc': 0.7678778612036532},\n",
       "              'LogisticRegression': {'accuracy': 0.9973053423155256,\n",
       "               'roc-auc': 0.9844570964480467},\n",
       "              'RandomForest': {'accuracy': 0.9971486761710794,\n",
       "               'roc-auc': 0.7584895151354426}},\n",
       "             'insult': {'NaiveBayes': {'accuracy': 0.9516841610527965,\n",
       "               'roc-auc': 0.8724510349051846},\n",
       "              'LogisticRegression': {'accuracy': 0.9691054363152123,\n",
       "               'roc-auc': 0.9725398296777387},\n",
       "              'RandomForest': {'accuracy': 0.9500548331505562,\n",
       "               'roc-auc': 0.8836885310448795}},\n",
       "             'identity_hate': {'NaiveBayes': {'accuracy': 0.9906000313332289,\n",
       "               'roc-auc': 0.8212709669188671},\n",
       "              'LogisticRegression': {'accuracy': 0.991414695284349,\n",
       "               'roc-auc': 0.9785415118406481},\n",
       "              'RandomForest': {'accuracy': 0.9906626977910074,\n",
       "               'roc-auc': 0.8899153840651097}}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
